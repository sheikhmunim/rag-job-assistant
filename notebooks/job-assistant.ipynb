{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7896f254-e40e-4010-b728-175cbc56ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT directory:        D:\\+Job\\rag-job-assistant\n",
      "DATA directory:        D:\\+Job\\rag-job-assistant\\data\n",
      "Outputs directory:     D:\\+Job\\rag-job-assistant\\data\\outputs\n",
      "RAG workspace:         D:\\+Job\\rag-job-assistant\\data\\job_rag\n",
      "Profile documents in:  D:\\+Job\\rag-job-assistant\\data\\job_rag\\profile_docs\n"
     ]
    }
   ],
   "source": [
    "# === EDIT ME ONCE ===\n",
    "USER_PROFILE = {\n",
    "    \"name\": \"Sheikh Abdul Munim\",\n",
    "    \"title\": \"AI/ML & Robotics Engineer\",\n",
    "    \"location\": \"Melbourne, Australia\",\n",
    "    \"email\": \"you@example.com\",\n",
    "    \"phone\": \"+61-4xx-xxx-xxx\",\n",
    "    \"links\": [\"https://github.com/SheikhMunim\", \"https://www.linkedin.com/in/your-handle\"],\n",
    "    \"skills\": [\n",
    "        \"Python\",\"PyTorch\",\"TensorFlow\",\"Jupyter\",\"LangChain\",\"RAG\",\"Transformers\",\"BERT\",\"Llama 3\",\"Ollama\",\n",
    "        \"Vector databases\",\"ChromaDB\",\"Docker\",\"FastAPI\",\"ROS2\",\"Gazebo\",\"PDDL\",\"Fast Downward\",\"PlanSys2\",\n",
    "        \"NLP\",\"Topic modeling\",\"Sentiment analysis\",\"OpenCV\",\"Communication\",\"Stakeholder collaboration\",\n",
    "        \"Technical writing\",\"Teaching/mentoring\"\n",
    "    ],\n",
    "    \"achievements\": [\n",
    "        \"Built a neural-symbolic pipeline combining BERT NLU with PDDL planning for service robots.\",\n",
    "        \"90%+ task success across 8 scenarios in simulation; validated on a real robot.\",\n",
    "        \"Multi-head intent parser with slot tagging for motion-level control in ROS2.\",\n",
    "        \"Containerized a RAG job-assistant; reproducible local GPU inference via Ollama.\"\n",
    "    ],\n",
    "    \"pitch\": (\n",
    "        \"I design reliable, explainable AI systems that combine strong language understanding with \"\n",
    "        \"symbolic planning to produce safe, human-aligned behavior.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Where your supporting docs live (resume, project notes, prior cover letters, etc.)\n",
    "from pathlib import Path\n",
    "\n",
    "# Move one level up from notebook folder (to project root)\n",
    "ROOT = Path.cwd().parent.resolve()\n",
    "\n",
    "# Define main data directories\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAG_DIR = DATA_DIR / \"job_rag\"\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROFILE_DIR = RAG_DIR / \"profile_docs\"\n",
    "PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT directory:       \", ROOT)\n",
    "print(\"DATA directory:       \", DATA_DIR)\n",
    "print(\"Outputs directory:    \", OUT_DIR)\n",
    "print(\"RAG workspace:        \", RAG_DIR)\n",
    "print(\"Profile documents in: \", PROFILE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193fa60c-cd3b-419c-aab8-4de4dfb499a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from collections import Counter\n",
    "import subprocess, sys\n",
    "\n",
    "def pip_install(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# NLTK + RapidFuzz\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    pip_install(\"nltk==3.9.1\"); import nltk\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "except ImportError:\n",
    "    pip_install(\"rapidfuzz==3.9.7\"); from rapidfuzz import process, fuzz\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True); nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = re.sub(r\"[\\r\\n]+\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def tokenize_lower(text: str):\n",
    "    toks = [t.lower() for t in word_tokenize(text)]\n",
    "    toks = [re.sub(r\"^\\W+|\\W+$\",\"\",t) for t in toks]\n",
    "    return [t for t in toks if t and t not in STOPWORDS and not t.isdigit()]\n",
    "\n",
    "HARD_SKILL_LEXICON = {\n",
    "    \"Python\",\"PyTorch\",\"TensorFlow\",\"NumPy\",\"Pandas\",\"scikit-learn\",\"Jupyter\",\n",
    "    \"Transformers\",\"BERT\",\"Llama\",\"RAG\",\"LangChain\",\"Ollama\",\"OpenAI\",\"Hugging Face\",\n",
    "    \"Vector DB\",\"Chroma\",\"FAISS\",\"Pinecone\",\"Weaviate\",\"Milvus\",\n",
    "    \"Docker\",\"FastAPI\",\"Flask\",\"REST API\",\"GraphQL\",\n",
    "    \"MLflow\",\"Weights & Biases\",\"W&B\",\"Ray\",\"Dask\",\n",
    "    \"LLM\",\"Prompt Engineering\",\"Reranking\",\"Guardrails\",\"Retrieval\",\"Chunking\",\n",
    "    \"CI/CD\",\"GCP\",\"AWS\",\"Azure\",\"Kubernetes\",\"GPU\",\"CUDA\",\n",
    "    \"ROS2\",\"Gazebo\",\"PDDL\",\"Fast Downward\",\"PlanSys2\",\"OpenCV\"\n",
    "}\n",
    "SOFT_SKILL_LEXICON = {\n",
    "    \"Communication\",\"Collaboration\",\"Leadership\",\"Problem solving\",\"Stakeholder management\",\n",
    "    \"Teamwork\",\"Time management\",\"Attention to detail\",\"Documentation\",\"Mentoring\",\"Ownership\"\n",
    "}\n",
    "\n",
    "def top_terms(tokens, topn=30, min_len=2):\n",
    "    c = Counter([t for t in tokens if len(t) >= min_len])\n",
    "    return [w for w,_ in c.most_common(topn)]\n",
    "\n",
    "def fuzzy_match_candidates(candidates, lexicon, cutoff=86):\n",
    "    found = set()\n",
    "    for cand in candidates:\n",
    "        match, score, _ = process.extractOne(cand, lexicon, scorer=fuzz.WRatio)\n",
    "        if score >= cutoff:\n",
    "            found.add(match)\n",
    "    return sorted(found)\n",
    "\n",
    "def bullet_list(items): \n",
    "    return \"\\n\".join([f\"- {x}\" for x in items])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cf1a7d-8c2a-46ea-9e23-f6826c5da843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Munim\\AppData\\Local\\Temp\\ipykernel_36848\\380620009.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": device})\n",
      "D:\\+Job\\rag-job-assistant\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\Munim\\AppData\\Local\\Temp\\ipykernel_36848\\380620009.py:28: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(embedding_function=emb, persist_directory=str(DB_DIR))\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ready | device: cuda | DB: D:\\+Job\\rag-job-assistant\\data\\job_rag\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "import torch, uuid\n",
    "\n",
    "DB_DIR = RAG_DIR / \"chroma_db\"; DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": device})\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150, separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "def load_docs_from(folder: Path, doc_type: str):\n",
    "    docs = []\n",
    "    for p in sorted(folder.glob(\"*\")):\n",
    "        if p.suffix.lower()==\".pdf\": docs += PyPDFLoader(str(p)).load()\n",
    "        elif p.suffix.lower() in {\".txt\",\".md\"}: docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = d.metadata.get(\"source\") or str(folder)\n",
    "        d.metadata[\"doc_type\"] = doc_type\n",
    "        d.metadata[\"uid\"] = str(uuid.uuid4())[:8]\n",
    "    return docs\n",
    "\n",
    "# build an empty persistent DB so we can reuse it\n",
    "vectordb = Chroma(embedding_function=emb, persist_directory=str(DB_DIR))\n",
    "retriever_all = vectordb.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "def index_profile_docs():\n",
    "    docs = load_docs_from(PROFILE_DIR, \"profile\")\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    if chunks:\n",
    "        vectordb.add_documents(chunks)\n",
    "    return len(chunks)\n",
    "\n",
    "def index_jd_text(jd_text: str):\n",
    "    # we index the JD fresh each time the final cell is run\n",
    "    tmp = (RAG_DIR / \"jd.txt\"); tmp.write_text(jd_text, encoding=\"utf-8\")\n",
    "    docs = load_docs_from(RAG_DIR, \"jd\")\n",
    "    # only keep the jd.txt content to avoid re-adding profile docs\n",
    "    docs = [d for d in docs if \"jd.txt\" in d.metadata.get(\"source\",\"\")]\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    if chunks:\n",
    "        vectordb.add_documents(chunks)\n",
    "    return len(chunks)\n",
    "\n",
    "def retrieve(query: str, k=6, doc_type=None):\n",
    "    docs = vectordb.similarity_search(query, k=24)\n",
    "    if doc_type:\n",
    "        docs = [d for d in docs if d.metadata.get(\"doc_type\")==doc_type]\n",
    "    return docs[:k]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"[{i+1}] {d.page_content}\" for i,d in enumerate(docs,1))\n",
    "\n",
    "def cite_sources(docs):\n",
    "    lines=[]\n",
    "    for i,d in enumerate(docs,1):\n",
    "        lines.append(f\"[{i}] {d.metadata.get('source','')}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# LLM connector\n",
    "import os\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\",\"http://localhost:11434\")\n",
    "LLM_MODEL   = os.getenv(\"LLM_MODEL\",\"llama3.2:3b\")\n",
    "llm = ChatOllama(base_url=OLLAMA_HOST, model=LLM_MODEL, temperature=0.3)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "def run_prompt(system_prompt: str, user_text: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "    chain = prompt | llm | parser\n",
    "    return chain.invoke({\"input\": user_text})\n",
    "\n",
    "print(\"RAG ready | device:\", device, \"| DB:\", DB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7edfc4b-42dd-4bc3-abb2-43d4ee731137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(profile: dict, jd_text: str, jd_snips: str, prof_snips: str,\n",
    "                  jd_hard, jd_soft, keywords, have_hard, have_soft, gaps):\n",
    "    return f\"\"\"\n",
    "[PROFILE]\n",
    "Name: {profile['name']} | Title: {profile['title']} | Location: {profile['location']}\n",
    "Email: {profile['email']} | Phone: {profile['phone']} | Links: {\", \".join(profile['links'])}\n",
    "\n",
    "Pitch:\n",
    "{profile['pitch']}\n",
    "\n",
    "Skills:\n",
    "{bullet_list(profile['skills'])}\n",
    "\n",
    "Achievements:\n",
    "{bullet_list(profile['achievements'])}\n",
    "\n",
    "[JOB_DESCRIPTION_RAW]\n",
    "{jd_text}\n",
    "\n",
    "[RETRIEVED_JD_SNIPPETS]\n",
    "{jd_snips}\n",
    "\n",
    "[RETRIEVED_PROFILE_SNIPPETS]\n",
    "{prof_snips}\n",
    "\n",
    "[EXTRACTED_FROM_JD]\n",
    "Hard skills: {\", \".join(jd_hard)}\n",
    "Soft skills: {\", \".join(jd_soft)}\n",
    "Extra keywords: {\", \".join(keywords[:30])}\n",
    "\n",
    "[ALIGNMENT_SUMMARY]\n",
    "You already have (hard): {\", \".join(have_hard)}\n",
    "You already have (soft): {\", \".join(have_soft)}\n",
    "Gaps to phrase carefully: {\", \".join(gaps)}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_SKILLS = \"\"\"\n",
    "You are a job-application copilot. From the context:\n",
    "1) HARD skills explicitly relevant to the JD and present in candidate/profile.\n",
    "2) SOFT skills tailored to the JD.\n",
    "3) 15–25 SEO keywords for CV/ATS.\n",
    "Rules:\n",
    "- Ground items in [RETRIEVED_*] where possible. No fabrications.\n",
    "- Use canonical names. Output as three sections with bullet lists.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_COVER = \"\"\"\n",
    "You are an expert cover-letter writer. Using the context:\n",
    "- Ground claims in [RETRIEVED_JD_SNIPPETS] and [RETRIEVED_PROFILE_SNIPPETS].\n",
    "- <=350 words, 3–5 short paragraphs + a 'Relevant Highlights' bullet list (3–5).\n",
    "- Quote exact JD terms where helpful. No invented experience.\n",
    "- Confident and specific; clear call-to-action.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_EMAILS = \"\"\"\n",
    "Write three short emails tailored to the JD and candidate:\n",
    "1) Application email (80–140 words) + 2–3 subject options.\n",
    "2) Cold recruiter outreach (40–80 words) + 2–3 subject options.\n",
    "3) Follow-up after 7–10 days (50–90 words) + 2–3 subject options.\n",
    "Ground skills in [RETRIEVED_*]. No exaggeration. Clean signature from [PROFILE].\n",
    "Format:\n",
    "=== Email 1 ===\n",
    "Subject: ...\n",
    "Body:\n",
    "...\n",
    "=== Email 2 ===\n",
    "...\n",
    "=== Email 3 ===\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_ATS = \"\"\"\n",
    "Create a compact ATS-friendly resume summary:\n",
    "- 3 bullets (outcomes-focused) aligned to JD.\n",
    "- One 'Core Stack' line (comma-separated tools).\n",
    "Keep to 80–120 words. Ground in [RETRIEVED_*]; no fabrications.\n",
    "\"\"\"\n",
    "\n",
    "def gen_skills(context): return run_prompt(SYSTEM_SKILLS, context)\n",
    "def gen_cover(context):  return run_prompt(SYSTEM_COVER,  context)\n",
    "def gen_emails(context): return run_prompt(SYSTEM_EMAILS, context)\n",
    "def gen_ats(context):    return run_prompt(SYSTEM_ATS,    context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314f3edd-c84d-49d3-a43f-27be6bf1c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\+Job\\rag-job-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SKILLS & KEYWORDS ===\n",
      " Here is the alignment summary:\n",
      "\n",
      "**You already have (hard):**\n",
      "\n",
      "* Python\n",
      "* PyTorch\n",
      "* TensorFlow\n",
      "* Jupyter\n",
      "* LangChain\n",
      "* RAG\n",
      "* Transformers\n",
      "* BERT\n",
      "* Llama 3\n",
      "* Ollama\n",
      "* Vector databases\n",
      "* ChromaDB\n",
      "* Docker\n",
      "* ROS2\n",
      "* Gazebo\n",
      "* PDDL\n",
      "* Fast Downward\n",
      "* PlanSys2\n",
      "* NLP\n",
      "* Topic modeling\n",
      "* Sentiment analysis\n",
      "* OpenCV\n",
      "\n",
      "**You already have (soft):**\n",
      "\n",
      "* Communication\n",
      "* Stakeholder collaboration\n",
      "* Technical writing\n",
      "* Teaching/mentoring\n",
      "\n",
      "**Gaps to phrase carefully:**\n",
      "\n",
      "* Experience developing and implementing deep learning algorithms, particularly with respect to computer vision algorithms\n",
      "* 5+ years of building machine learning models or developing algorithms for business application experience\n",
      "* Experience using Unix/Linux\n",
      "* Experience with popular deep learning frameworks such as MxNet and Tensor Flow\n",
      "\n",
      "**15–25 SEO keywords for CV/ATS:**\n",
      "\n",
      "* Machine learning engineer\n",
      "* Computer vision scientist\n",
      "* AI researcher\n",
      "* Deep learning expert\n",
      "* Python developer\n",
      "* PyTorch developer\n",
      "* TensorFlow developer\n",
      "* ROS2 developer\n",
      "* Gazebo developer\n",
      "* NLP engineer\n",
      "* Natural language processing\n",
      "* Computer vision algorithms\n",
      "* Deep learning frameworks\n",
      "* Amazon Web Services (AWS)\n",
      "* Cloud computing\n",
      "* Data analysis\n",
      "* Data science\n",
      "* Artificial intelligence engineer \n",
      "\n",
      "=== COVER LETTER ===\n",
      " Here is a cover letter tailored to the provided context:\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am thrilled to apply for the Computer Vision Scientist position at Amazon, where I can leverage my expertise in AI/ML and robotics to develop innovative solutions that drive business success. With a strong foundation in computer vision, machine learning, and programming languages like Python, PyTorch, and TensorFlow, I am confident in my ability to contribute to the development of scalable machine learning and computer vision solutions.\n",
      "\n",
      "As a seasoned researcher and engineer with experience in designing reliable AI systems that combine strong language understanding with symbolic planning, I possess a unique blend of technical expertise and business acumen. My achievements in building neural-symbolic pipelines, developing generative AI models, and conducting market research to solve customer problems align perfectly with the requirements of this role.\n",
      "\n",
      "I am particularly drawn to Amazon's mission to create next-generation media content that delivers an unforgettable shopping experience for customers in emerging markets. I am excited about the opportunity to join a team dedicated to advancing AI technology and transforming it into impactful business solutions.\n",
      "\n",
      "Throughout my academic and professional journey, I have developed strong skills in communication, stakeholder collaboration, technical writing, and teaching/mentoring. My experience as a group project leader and electronic team lead has also honed my ability to work effectively in teams and manage complex projects.\n",
      "\n",
      "Relevant Highlights:\n",
      "\n",
      "* Designed reliable AI systems that combine strong language understanding with symbolic planning\n",
      "* Developed generative AI models tailored to business use cases\n",
      "* Conducted market research and implemented novel machine learning techniques to solve customer problems\n",
      "* Published papers at Tier-1 CV/ML conferences\n",
      "* Mentored interns and junior scientists\n",
      "\n",
      "I am excited about the opportunity to bring my expertise and passion for computer vision and AI to Amazon. Thank you for considering my application.\n",
      "\n",
      "Sincerely,\n",
      "Sheikh Abdul Munim \n",
      "\n",
      "=== EMAILS ===\n",
      " Here are the three emails tailored to the JD and candidate:\n",
      "\n",
      "**Email 1: Application Email**\n",
      "\n",
      "Subject Options:\n",
      "* \"Exciting Opportunity in AI/ML & Robotics Engineering\"\n",
      "* \"Join Our Team as a Computer Vision Scientist\"\n",
      "* \"Revolutionize Media Content with Your Expertise\"\n",
      "\n",
      "Body:\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am writing to express my strong interest in the Computer Vision Scientist position at Amazon. As an AI/ML & Robotics Engineer with a passion for developing reliable and explainable AI systems, I am confident that I can make a significant contribution to your team.\n",
      "\n",
      "With [number] years of experience in programming languages like Python, PyTorch, and TensorFlow, I possess the technical skills required to develop scalable machine learning and computer vision solutions. My expertise in ROS2, Gazebo, and PDDL planning will enable me to design and build innovative AI models tailored to your business use cases.\n",
      "\n",
      "I am particularly drawn to this role because of the opportunity to work on projects that tackle real-world business challenges and deliver an unforgettable shopping experience for customers. I am excited about the prospect of collaborating with stakeholders to understand and align on requirements, as well as mentoring interns and junior scientists.\n",
      "\n",
      "Please find my resume attached. I would be thrilled to discuss my application and how I can contribute to Amazon's mission.\n",
      "\n",
      "Best regards,\n",
      "Sheikh Abdul Munim\n",
      "\n",
      "**Email 2: Cold Recruiter Outreach**\n",
      "\n",
      "Subject Options:\n",
      "* \"Exploring Opportunities in AI/ML & Robotics Engineering\"\n",
      "* \"Computer Vision Scientist Role at Amazon - Let's Discuss\"\n",
      "* \"Unlock Your Potential as a Computer Vision Expert\"\n",
      "\n",
      "Body:\n",
      "\n",
      "Hi [Hiring Manager],\n",
      "\n",
      "I came across the Computer Vision Scientist position at Amazon and was impressed by the company's mission to create next-generation media content. As an AI/ML & Robotics Engineer with expertise in computer vision, I believe I would be a great fit for this role.\n",
      "\n",
      "With my strong background in programming languages like Python, PyTorch, and TensorFlow, I am confident that I can develop scalable machine learning and computer vision solutions. My experience with ROS2, Gazebo, and PDDL planning will enable me to design and build innovative AI models tailored to your business use cases.\n",
      "\n",
      "I would love to discuss this opportunity further and explore how my skills align with Amazon's goals. Please let me know if you're available for a call or meeting.\n",
      "\n",
      "Best regards,\n",
      "Sheikh Abdul Munim\n",
      "\n",
      "**Email 3: Follow-up after 7-10 days**\n",
      "\n",
      "Subject Options:\n",
      "* \"Following up on Computer Vision Scientist Opportunity\"\n",
      "* \"Next Steps in Our Conversation about AI/ML & Robotics Engineering\"\n",
      "* \"Revisiting the Computer Vision Scientist Role at Amazon\"\n",
      "\n",
      "Body:\n",
      "\n",
      "Hi [Hiring Manager],\n",
      "\n",
      "I hope this email finds you well. I wanted to follow up on our previous conversation regarding the Computer Vision Scientist position at Amazon. I am still very interested in exploring how my skills and experience can contribute to your team.\n",
      "\n",
      "If there's any additional information or materials that I can provide to support my application, please let me know. I would be more than happy to share my resume, portfolio, or any other relevant documents.\n",
      "\n",
      "Thank you for considering my application. I look forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "Sheikh Abdul Munim \n",
      "\n",
      "=== ATS SUMMARY ===\n",
      " Here's a compact ATS-friendly resume summary:\n",
      "\n",
      "**AI/ML & Robotics Engineer**\n",
      "\n",
      "- Designed reliable neural-symbolic pipelines for service robots, achieving 90%+ task success in simulation and real-world validation.\n",
      "- Developed multi-head intent parsers with slot tagging for motion-level control in ROS2, improving autonomous navigation.\n",
      "- Containerized RAG job-assistants using Ollama for reproducible local GPU inference.\n",
      "\n",
      "**Core Stack:** PyTorch, TensorFlow, ROS2, Docker, FastAPI, OpenCV.\n",
      "\n",
      "This summary highlights your key achievements and technical skills, while keeping it concise and ATS-friendly. It also aligns with the job description's requirements and emphasizes your expertise in AI/ML and robotics engineering. \n",
      "\n",
      "Saved files to: D:\\+Job\\rag-job-assistant\\data\\outputs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 1) paste JD text here and run this cell only:\n",
    "JD_INPUT = \"\"\"\n",
    "PASTE THE FULL JOB DESCRIPTION HERE.\n",
    "\"\"\"\n",
    "\n",
    "# 2) index JD (fresh) and ensure your profile docs are indexed once\n",
    "_ = index_profile_docs()\n",
    "_ = index_jd_text(JD_INPUT)\n",
    "\n",
    "# 3) retrieve focused snippets\n",
    "jd_focus     = retrieve(\"List must-have requirements and responsibilities.\", k=6, doc_type=\"jd\")\n",
    "profile_focus= retrieve(\"Find bullets that prove impact, results, metrics.\", k=6, doc_type=\"profile\")\n",
    "RAG_JD       = format_docs(jd_focus)\n",
    "RAG_PROFILE  = format_docs(profile_focus)\n",
    "\n",
    "# 4) keyword/skill extraction from JD text\n",
    "JD_N = normalize_text(JD_INPUT)\n",
    "TOKS = tokenize_lower(JD_N)\n",
    "cands = top_terms(TOKS, topn=80, min_len=2)\n",
    "jd_hard = fuzzy_match_candidates(cands, HARD_SKILL_LEXICON, cutoff=86)\n",
    "jd_soft = fuzzy_match_candidates(cands, SOFT_SKILL_LEXICON, cutoff=86)\n",
    "\n",
    "caps = sorted(set(re.findall(r\"\\b([A-Z][a-zA-Z0-9\\-\\+&/]{1,})\\b\", JD_INPUT)))\n",
    "extra = fuzzy_match_candidates([c.lower() for c in caps], {s.lower():s for s in HARD_SKILL_LEXICON}.keys(), cutoff=90)\n",
    "extra_cased = [next((x for x in HARD_SKILL_LEXICON if x.lower()==e), e) for e in extra]\n",
    "jd_hard = sorted(set(jd_hard) | set(extra_cased))\n",
    "\n",
    "known = {t.lower() for t in jd_hard + jd_soft}\n",
    "keywords = [t for t in cands if t not in known and len(t) >= 3]\n",
    "\n",
    "# 5) alignment summary\n",
    "from rapidfuzz import process, fuzz\n",
    "def fuzzy_overlap(yours, theirs, cutoff=88):\n",
    "    matches = []\n",
    "    for y in yours:\n",
    "        m = process.extractOne(y, theirs, scorer=fuzz.WRatio)\n",
    "        if m and m[1] >= cutoff:\n",
    "            matches.append((y, m[0], m[1]))\n",
    "    return matches\n",
    "\n",
    "have_hard = sorted({m[1] for m in fuzzy_overlap(USER_PROFILE[\"skills\"], jd_hard, 88)})\n",
    "have_soft = sorted({m[1] for m in fuzzy_overlap(USER_PROFILE[\"skills\"], jd_soft, 88)})\n",
    "gaps = [s for s in jd_hard+jd_soft if s not in set(have_hard+have_soft)]\n",
    "\n",
    "# 6) build context and generate all outputs\n",
    "CTX = build_context(USER_PROFILE, JD_INPUT, RAG_JD, RAG_PROFILE, jd_hard, jd_soft, keywords, have_hard, have_soft, gaps)\n",
    "\n",
    "skills_out = gen_skills(CTX)\n",
    "cover_out  = gen_cover(CTX)\n",
    "emails_out = gen_emails(CTX)\n",
    "ats_out    = gen_ats(CTX)\n",
    "\n",
    "# 7) show and save\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "(OUT_DIR / f\"{ts}_skills_keywords.md\").write_text(skills_out, encoding=\"utf-8\")\n",
    "(OUT_DIR / f\"{ts}_cover_letter.md\").write_text(cover_out, encoding=\"utf-8\")\n",
    "(OUT_DIR / f\"{ts}_emails.md\").write_text(emails_out, encoding=\"utf-8\")\n",
    "(OUT_DIR / f\"{ts}_ats_summary.md\").write_text(ats_out, encoding=\"utf-8\")\n",
    "\n",
    "print(\"=== SKILLS & KEYWORDS ===\\n\", skills_out, \"\\n\")\n",
    "print(\"=== COVER LETTER ===\\n\", cover_out, \"\\n\")\n",
    "print(\"=== EMAILS ===\\n\", emails_out, \"\\n\")\n",
    "print(\"=== ATS SUMMARY ===\\n\", ats_out, \"\\n\")\n",
    "print(\"Saved files to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48a1d7-993c-47a4-8aa5-75ca285f27c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
