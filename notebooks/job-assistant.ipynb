{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7896f254-e40e-4010-b728-175cbc56ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT directory:        D:\\+Job\\rag-job-assistant\n",
      "DATA directory:        D:\\+Job\\rag-job-assistant\\data\n",
      "Outputs directory:     D:\\+Job\\rag-job-assistant\\data\\outputs\n",
      "RAG workspace:         D:\\+Job\\rag-job-assistant\\data\\job_rag\n",
      "Profile documents in:  D:\\+Job\\rag-job-assistant\\data\\job_rag\\profile_docs\n"
     ]
    }
   ],
   "source": [
    "# === EDIT ME ONCE ===\n",
    "USER_PROFILE = {\n",
    "    \"name\": \"Sheikh Abdul Munim\",\n",
    "    \"title\": \"AI/ML & Robotics Engineer\",\n",
    "    \"location\": \"Melbourne, Australia\",\n",
    "    \"email\": \"you@example.com\",\n",
    "    \"phone\": \"+61-4xx-xxx-xxx\",\n",
    "    \"links\": [\"https://github.com/SheikhMunim\", \"https://www.linkedin.com/in/your-handle\"],\n",
    "    \"skills\": [\n",
    "        \"Python\",\"PyTorch\",\"TensorFlow\",\"Jupyter\",\"LangChain\",\"RAG\",\"Transformers\",\"BERT\",\"Llama 3\",\"Ollama\",\n",
    "        \"Vector databases\",\"ChromaDB\",\"Docker\",\"FastAPI\",\"ROS2\",\"Gazebo\",\"PDDL\",\"Fast Downward\",\"PlanSys2\",\n",
    "        \"NLP\",\"Topic modeling\",\"Sentiment analysis\",\"OpenCV\",\"Communication\",\"Stakeholder collaboration\",\n",
    "        \"Technical writing\",\"Teaching/mentoring\"\n",
    "    ],\n",
    "    \"achievements\": [\n",
    "        \"Built a neural-symbolic pipeline combining BERT NLU with PDDL planning for service robots.\",\n",
    "        \"90%+ task success across 8 scenarios in simulation; validated on a real robot.\",\n",
    "        \"Multi-head intent parser with slot tagging for motion-level control in ROS2.\",\n",
    "        \"Containerized a RAG job-assistant; reproducible local GPU inference via Ollama.\"\n",
    "    ],\n",
    "    \"pitch\": (\n",
    "        \"I design reliable, explainable AI systems that combine strong language understanding with \"\n",
    "        \"symbolic planning to produce safe, human-aligned behavior.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Where your supporting docs live (resume, project notes, prior cover letters, etc.)\n",
    "from pathlib import Path\n",
    "\n",
    "# Move one level up from notebook folder (to project root)\n",
    "ROOT = Path.cwd().parent.resolve()\n",
    "\n",
    "# Define main data directories\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAG_DIR = DATA_DIR / \"job_rag\"\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROFILE_DIR = RAG_DIR / \"profile_docs\"\n",
    "PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT directory:       \", ROOT)\n",
    "print(\"DATA directory:       \", DATA_DIR)\n",
    "print(\"Outputs directory:    \", OUT_DIR)\n",
    "print(\"RAG workspace:        \", RAG_DIR)\n",
    "print(\"Profile documents in: \", PROFILE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193fa60c-cd3b-419c-aab8-4de4dfb499a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from collections import Counter\n",
    "import subprocess, sys\n",
    "\n",
    "def pip_install(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# NLTK + RapidFuzz\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    pip_install(\"nltk==3.9.1\"); import nltk\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "except ImportError:\n",
    "    pip_install(\"rapidfuzz==3.9.7\"); from rapidfuzz import process, fuzz\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True); nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = re.sub(r\"[\\r\\n]+\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def tokenize_lower(text: str):\n",
    "    toks = [t.lower() for t in word_tokenize(text)]\n",
    "    toks = [re.sub(r\"^\\W+|\\W+$\",\"\",t) for t in toks]\n",
    "    return [t for t in toks if t and t not in STOPWORDS and not t.isdigit()]\n",
    "\n",
    "HARD_SKILL_LEXICON = {\n",
    "    \"Python\",\"PyTorch\",\"TensorFlow\",\"NumPy\",\"Pandas\",\"scikit-learn\",\"Jupyter\",\n",
    "    \"Transformers\",\"BERT\",\"Llama\",\"RAG\",\"LangChain\",\"Ollama\",\"OpenAI\",\"Hugging Face\",\n",
    "    \"Vector DB\",\"Chroma\",\"FAISS\",\"Pinecone\",\"Weaviate\",\"Milvus\",\n",
    "    \"Docker\",\"FastAPI\",\"Flask\",\"REST API\",\"GraphQL\",\n",
    "    \"MLflow\",\"Weights & Biases\",\"W&B\",\"Ray\",\"Dask\",\n",
    "    \"LLM\",\"Prompt Engineering\",\"Reranking\",\"Guardrails\",\"Retrieval\",\"Chunking\",\n",
    "    \"CI/CD\",\"GCP\",\"AWS\",\"Azure\",\"Kubernetes\",\"GPU\",\"CUDA\",\n",
    "    \"ROS2\",\"Gazebo\",\"PDDL\",\"Fast Downward\",\"PlanSys2\",\"OpenCV\"\n",
    "}\n",
    "SOFT_SKILL_LEXICON = {\n",
    "    \"Communication\",\"Collaboration\",\"Leadership\",\"Problem solving\",\"Stakeholder management\",\n",
    "    \"Teamwork\",\"Time management\",\"Attention to detail\",\"Documentation\",\"Mentoring\",\"Ownership\"\n",
    "}\n",
    "\n",
    "def top_terms(tokens, topn=30, min_len=2):\n",
    "    c = Counter([t for t in tokens if len(t) >= min_len])\n",
    "    return [w for w,_ in c.most_common(topn)]\n",
    "\n",
    "def fuzzy_match_candidates(candidates, lexicon, cutoff=86):\n",
    "    found = set()\n",
    "    for cand in candidates:\n",
    "        match, score, _ = process.extractOne(cand, lexicon, scorer=fuzz.WRatio)\n",
    "        if score >= cutoff:\n",
    "            found.add(match)\n",
    "    return sorted(found)\n",
    "\n",
    "def bullet_list(items): \n",
    "    return \"\\n\".join([f\"- {x}\" for x in items])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cf1a7d-8c2a-46ea-9e23-f6826c5da843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Munim\\AppData\\Local\\Temp\\ipykernel_29388\\380620009.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": device})\n",
      "D:\\+Job\\rag-job-assistant\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\Munim\\AppData\\Local\\Temp\\ipykernel_29388\\380620009.py:28: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(embedding_function=emb, persist_directory=str(DB_DIR))\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ready | device: cuda | DB: D:\\+Job\\rag-job-assistant\\data\\job_rag\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "import torch, uuid\n",
    "\n",
    "DB_DIR = RAG_DIR / \"chroma_db\"; DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": device})\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150, separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "\n",
    "def load_docs_from(folder: Path, doc_type: str):\n",
    "    docs = []\n",
    "    for p in sorted(folder.glob(\"*\")):\n",
    "        if p.suffix.lower()==\".pdf\": docs += PyPDFLoader(str(p)).load()\n",
    "        elif p.suffix.lower() in {\".txt\",\".md\"}: docs += TextLoader(str(p), encoding=\"utf-8\").load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = d.metadata.get(\"source\") or str(folder)\n",
    "        d.metadata[\"doc_type\"] = doc_type\n",
    "        d.metadata[\"uid\"] = str(uuid.uuid4())[:8]\n",
    "    return docs\n",
    "\n",
    "# build an empty persistent DB so we can reuse it\n",
    "vectordb = Chroma(embedding_function=emb, persist_directory=str(DB_DIR))\n",
    "retriever_all = vectordb.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "def index_profile_docs():\n",
    "    docs = load_docs_from(PROFILE_DIR, \"profile\")\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    if chunks:\n",
    "        vectordb.add_documents(chunks)\n",
    "    return len(chunks)\n",
    "\n",
    "def index_jd_text(jd_text: str):\n",
    "    # we index the JD fresh each time the final cell is run\n",
    "    tmp = (RAG_DIR / \"jd.txt\"); tmp.write_text(jd_text, encoding=\"utf-8\")\n",
    "    docs = load_docs_from(RAG_DIR, \"jd\")\n",
    "    # only keep the jd.txt content to avoid re-adding profile docs\n",
    "    docs = [d for d in docs if \"jd.txt\" in d.metadata.get(\"source\",\"\")]\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    if chunks:\n",
    "        vectordb.add_documents(chunks)\n",
    "    return len(chunks)\n",
    "\n",
    "def retrieve(query: str, k=6, doc_type=None):\n",
    "    docs = vectordb.similarity_search(query, k=24)\n",
    "    if doc_type:\n",
    "        docs = [d for d in docs if d.metadata.get(\"doc_type\")==doc_type]\n",
    "    return docs[:k]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"[{i+1}] {d.page_content}\" for i,d in enumerate(docs,1))\n",
    "\n",
    "def cite_sources(docs):\n",
    "    lines=[]\n",
    "    for i,d in enumerate(docs,1):\n",
    "        lines.append(f\"[{i}] {d.metadata.get('source','')}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# LLM connector\n",
    "import os\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\",\"http://localhost:11434\")\n",
    "LLM_MODEL   = os.getenv(\"LLM_MODEL\",\"llama3.2:3b\")\n",
    "llm = ChatOllama(base_url=OLLAMA_HOST, model=LLM_MODEL, temperature=0.3)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "def run_prompt(system_prompt: str, user_text: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "    chain = prompt | llm | parser\n",
    "    return chain.invoke({\"input\": user_text})\n",
    "\n",
    "print(\"RAG ready | device:\", device, \"| DB:\", DB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7edfc4b-42dd-4bc3-abb2-43d4ee731137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(profile: dict, jd_text: str, jd_snips: str, prof_snips: str,\n",
    "                  jd_hard, jd_soft, keywords, have_hard, have_soft, gaps):\n",
    "    return f\"\"\"\n",
    "[PROFILE]\n",
    "Name: {profile['name']} | Title: {profile['title']} | Location: {profile['location']}\n",
    "Email: {profile['email']} | Phone: {profile['phone']} | Links: {\", \".join(profile['links'])}\n",
    "\n",
    "Pitch:\n",
    "{profile['pitch']}\n",
    "\n",
    "Skills:\n",
    "{bullet_list(profile['skills'])}\n",
    "\n",
    "Achievements:\n",
    "{bullet_list(profile['achievements'])}\n",
    "\n",
    "[JOB_DESCRIPTION_RAW]\n",
    "{jd_text}\n",
    "\n",
    "[RETRIEVED_JD_SNIPPETS]\n",
    "{jd_snips}\n",
    "\n",
    "[RETRIEVED_PROFILE_SNIPPETS]\n",
    "{prof_snips}\n",
    "\n",
    "[EXTRACTED_FROM_JD]\n",
    "Hard skills: {\", \".join(jd_hard)}\n",
    "Soft skills: {\", \".join(jd_soft)}\n",
    "Extra keywords: {\", \".join(keywords[:30])}\n",
    "\n",
    "[ALIGNMENT_SUMMARY]\n",
    "You already have (hard): {\", \".join(have_hard)}\n",
    "You already have (soft): {\", \".join(have_soft)}\n",
    "Gaps to phrase carefully: {\", \".join(gaps)}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_SKILLS = \"\"\"\n",
    "You are a job-application copilot. From the context:\n",
    "1) HARD skills explicitly relevant to the JD and present in candidate/profile.\n",
    "2) SOFT skills tailored to the JD.\n",
    "3) 15–25 SEO keywords for CV/ATS.\n",
    "Rules:\n",
    "- Ground items in [RETRIEVED_*] where possible. No fabrications.\n",
    "- Use canonical names. Output as three sections with bullet lists.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_COVER = \"\"\"\n",
    "You are an expert cover-letter writer. Using the context:\n",
    "- Ground claims in [RETRIEVED_JD_SNIPPETS] and [RETRIEVED_PROFILE_SNIPPETS].\n",
    "- <=350 words, 3–5 short paragraphs + a 'Relevant Highlights' bullet list (3–5).\n",
    "- Quote exact JD terms where helpful. No invented experience.\n",
    "- Confident and specific; clear call-to-action.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_EMAILS = \"\"\"\n",
    "Write three short emails tailored to the JD and candidate:\n",
    "1) Application email (80–140 words) + 2–3 subject options.\n",
    "2) Cold recruiter outreach (40–80 words) + 2–3 subject options.\n",
    "3) Follow-up after 7–10 days (50–90 words) + 2–3 subject options.\n",
    "Ground skills in [RETRIEVED_*]. No exaggeration. Clean signature from [PROFILE].\n",
    "Format:\n",
    "=== Email 1 ===\n",
    "Subject: ...\n",
    "Body:\n",
    "...\n",
    "=== Email 2 ===\n",
    "...\n",
    "=== Email 3 ===\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_ATS = \"\"\"\n",
    "Create a compact ATS-friendly resume summary:\n",
    "- 3 bullets (outcomes-focused) aligned to JD.\n",
    "- One 'Core Stack' line (comma-separated tools).\n",
    "Keep to 80–120 words. Ground in [RETRIEVED_*]; no fabrications.\n",
    "\"\"\"\n",
    "\n",
    "def gen_skills(context): return run_prompt(SYSTEM_SKILLS, context)\n",
    "def gen_cover(context):  return run_prompt(SYSTEM_COVER,  context)\n",
    "def gen_emails(context): return run_prompt(SYSTEM_EMAILS, context)\n",
    "def gen_ats(context):    return run_prompt(SYSTEM_ATS,    context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314f3edd-c84d-49d3-a43f-27be6bf1c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\+Job\\rag-job-assistant\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SKILLS & KEYWORDS ===\n",
      " **Hard Skills**\n",
      "\n",
      "* Python\n",
      "* PyTorch\n",
      "* TensorFlow\n",
      "* Jupyter\n",
      "* LangChain\n",
      "* RAG\n",
      "* Transformers\n",
      "* BERT\n",
      "* Llama 3\n",
      "* Ollama\n",
      "* Vector databases\n",
      "* ChromaDB\n",
      "* Docker\n",
      "* FastAPI\n",
      "* ROS2\n",
      "* Gazebo\n",
      "* PDDL\n",
      "* Fast Downward\n",
      "* PlanSys2\n",
      "* NLP\n",
      "* Topic modeling\n",
      "* Sentiment analysis\n",
      "* OpenCV\n",
      "\n",
      "**Soft Skills**\n",
      "\n",
      "* Communication\n",
      "* Stakeholder collaboration\n",
      "* Technical writing\n",
      "* Teaching/mentoring\n",
      "\n",
      "**15–25 SEO Keywords for CV/ATS**\n",
      "\n",
      "* Machine learning engineer\n",
      "* Computer vision scientist\n",
      "* Generative AI models\n",
      "* Deep learning frameworks\n",
      "* Python programming\n",
      "* ROS2 development\n",
      "* Docker containerization\n",
      "* Cloud computing resources\n",
      "* Amazon data analysis\n",
      "* Content automation\n",
      "* Business stakeholder collaboration\n",
      "* Research and development\n",
      "* Novel machine learning techniques\n",
      "* Tier-1 CV/ML conferences \n",
      "\n",
      "=== COVER LETTER ===\n",
      " Here's a cover letter tailored to the job description:\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am thrilled to apply for the Computer Vision Scientist position at Amazon Australia, where I can leverage my expertise in AI/ML and robotics engineering to develop innovative solutions that transform the next-generation e-commerce platform. With a strong foundation in programming languages like Python, PyTorch, and TensorFlow, I am confident in my ability to design, build, and evaluate generative AI models tailored to your business use cases.\n",
      "\n",
      "As a seasoned expert in computer vision and machine learning, I have a proven track record of delivering reliable and explainable AI systems that combine strong language understanding with symbolic planning. My experience with popular deep learning frameworks such as MxNet and Tensor Flow has equipped me with the skills to develop state-of-the-art models for computer vision applications.\n",
      "\n",
      "I am particularly drawn to this role because of the opportunity to work on a high-performing Computer Vision team at Amazon, where I can contribute to developing generative AI solutions that leverage vast amounts of Amazon data and advanced cloud computing resources. My experience in ROS2, Gazebo, and PDDL planning has given me a solid understanding of the importance of integrating computer vision with robotic systems.\n",
      "\n",
      "As a mentor and educator, I have led global coding courses and taught first half of Stanford's CS106A course using Python as the primary language. I am confident in my ability to communicate clearly with business stakeholders to understand and align on requirements, and I am excited about the opportunity to collaborate with a talented team of professionals.\n",
      "\n",
      "Relevant Highlights:\n",
      "\n",
      "* Designed and developed a neural-symbolic pipeline combining BERT NLU with PDDL planning for service robots, achieving 90%+ task success across 8 scenarios in simulation.\n",
      "* Built a multi-head intent parser with slot tagging for motion-level control in ROS2.\n",
      "* Containerized a RAG job-assistant; reproducible local GPU inference via Ollama.\n",
      "\n",
      "I am excited about the opportunity to join Amazon Australia and contribute my skills and experience to developing innovative AI solutions that transform the next-generation e-commerce platform. Thank you for considering my application.\n",
      "\n",
      "Sincerely,\n",
      "Sheikh Abdul Munim \n",
      "\n",
      "=== EMAILS ===\n",
      " Here are three short emails tailored to the JD and candidate:\n",
      "\n",
      "**Email 1: Application**\n",
      "\n",
      "Subject: Application for Computer Vision Scientist at Amazon Australia\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am excited to apply for the Computer Vision Scientist position at Amazon Australia. With my strong background in AI, ML, and robotics engineering, I am confident that I can contribute to developing innovative AI techniques that tackle real-world business challenges.\n",
      "\n",
      "As a seasoned engineer with experience in Python, PyTorch, TensorFlow, and ROS2, I possess the technical skills required for this role. My achievements in building neural-symbolic pipelines, generative models, and containerizing RAG job-assistants demonstrate my ability to design, build, and evaluate AI systems.\n",
      "\n",
      "I am particularly drawn to this role because of the opportunity to work with vast amounts of Amazon data and advanced cloud computing resources. I am excited about the prospect of joining a high-performing team and contributing to the development of generative AI solutions that transform the next generation of e-commerce platform.\n",
      "\n",
      "Thank you for considering my application. I look forward to discussing my qualifications further.\n",
      "\n",
      "Best regards,\n",
      "Sheikh Abdul Munim\n",
      "\n",
      "**Email 2: Cold Recruiter Outreach**\n",
      "\n",
      "Subject: Exploring Opportunities in Computer Vision at Amazon Australia\n",
      "\n",
      "Hi [Hiring Manager's Name],\n",
      "\n",
      "I came across the Computer Vision Scientist position at Amazon Australia and was impressed by the role's focus on developing innovative AI techniques. As a seasoned engineer with expertise in AI, ML, and robotics engineering, I thought you might be interested in exploring potential opportunities.\n",
      "\n",
      "With my background in Python, PyTorch, TensorFlow, and ROS2, I possess the technical skills required for this role. My achievements in building neural-symbolic pipelines, generative models, and containerizing RAG job-assistants demonstrate my ability to design, build, and evaluate AI systems.\n",
      "\n",
      "If you're open to discussing potential opportunities, I'd love to schedule a call to explore how my skills align with the team's needs.\n",
      "\n",
      "Best regards,\n",
      "Sheikh Abdul Munim\n",
      "\n",
      "**Email 3: Follow-up after 7-10 days**\n",
      "\n",
      "Subject: Following up on Computer Vision Scientist Application\n",
      "\n",
      "Hi [Hiring Manager's Name],\n",
      "\n",
      "I hope this email finds you well. It's been [X] weeks since I applied for the Computer Vision Scientist position at Amazon Australia, and I wanted to follow up on the status of my application.\n",
      "\n",
      "I understand that hiring processes can take time, but I'm still very interested in exploring opportunities with your team. If there's any update on the progress or if you'd like to schedule a call to discuss further, please let me know.\n",
      "\n",
      "Thank you for your time and consideration.\n",
      "\n",
      "Best regards,\n",
      "Sheikh Abdul Munim \n",
      "\n",
      "=== ATS SUMMARY ===\n",
      " Here's a compact ATS-friendly resume summary:\n",
      "\n",
      "**Reliable AI/ML & Robotics Engineer**\n",
      "\n",
      "Designs safe, human-aligned behavior through neural-symbolic pipelines, leveraging strong language understanding and symbolic planning. Develops scalable machine learning solutions with computer vision capabilities, driving business value through automation and insights.\n",
      "\n",
      "**Core Stack:** PyTorch, TensorFlow, ROS2, Docker, FastAPI\n",
      "\n",
      "This summary highlights your key strengths in AI/ML and robotics engineering, while also showcasing your technical skills and expertise. The alignment summary section helps you identify areas to phrase carefully or fill gaps, ensuring a strong ATS-friendly resume. \n",
      "\n",
      "Saved files to: D:\\+Job\\rag-job-assistant\\data\\outputs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 1) paste JD text here and run this cell only:\n",
    "JD_INPUT = \"\"\"\n",
    "PASTE THE FULL JOB DESCRIPTION HERE.\n",
    "\"\"\"\n",
    "\n",
    "# 2) index JD (fresh) and ensure your profile docs are indexed once\n",
    "_ = index_profile_docs()\n",
    "_ = index_jd_text(JD_INPUT)\n",
    "\n",
    "# 3) retrieve focused snippets\n",
    "jd_focus     = retrieve(\"List must-have requirements and responsibilities.\", k=6, doc_type=\"jd\")\n",
    "profile_focus= retrieve(\"Find bullets that prove impact, results, metrics.\", k=6, doc_type=\"profile\")\n",
    "RAG_JD       = format_docs(jd_focus)\n",
    "RAG_PROFILE  = format_docs(profile_focus)\n",
    "\n",
    "# 4) keyword/skill extraction from JD text\n",
    "JD_N = normalize_text(JD_INPUT)\n",
    "TOKS = tokenize_lower(JD_N)\n",
    "cands = top_terms(TOKS, topn=80, min_len=2)\n",
    "jd_hard = fuzzy_match_candidates(cands, HARD_SKILL_LEXICON, cutoff=86)\n",
    "jd_soft = fuzzy_match_candidates(cands, SOFT_SKILL_LEXICON, cutoff=86)\n",
    "\n",
    "caps = sorted(set(re.findall(r\"\\b([A-Z][a-zA-Z0-9\\-\\+&/]{1,})\\b\", JD_INPUT)))\n",
    "extra = fuzzy_match_candidates([c.lower() for c in caps], {s.lower():s for s in HARD_SKILL_LEXICON}.keys(), cutoff=90)\n",
    "extra_cased = [next((x for x in HARD_SKILL_LEXICON if x.lower()==e), e) for e in extra]\n",
    "jd_hard = sorted(set(jd_hard) | set(extra_cased))\n",
    "\n",
    "known = {t.lower() for t in jd_hard + jd_soft}\n",
    "keywords = [t for t in cands if t not in known and len(t) >= 3]\n",
    "\n",
    "# 5) alignment summary\n",
    "from rapidfuzz import process, fuzz\n",
    "def fuzzy_overlap(yours, theirs, cutoff=88):\n",
    "    matches = []\n",
    "    for y in yours:\n",
    "        m = process.extractOne(y, theirs, scorer=fuzz.WRatio)\n",
    "        if m and m[1] >= cutoff:\n",
    "            matches.append((y, m[0], m[1]))\n",
    "    return matches\n",
    "\n",
    "have_hard = sorted({m[1] for m in fuzzy_overlap(USER_PROFILE[\"skills\"], jd_hard, 88)})\n",
    "have_soft = sorted({m[1] for m in fuzzy_overlap(USER_PROFILE[\"skills\"], jd_soft, 88)})\n",
    "gaps = [s for s in jd_hard+jd_soft if s not in set(have_hard+have_soft)]\n",
    "\n",
    "# 6) build context and generate all outputs\n",
    "CTX = build_context(USER_PROFILE, JD_INPUT, RAG_JD, RAG_PROFILE, jd_hard, jd_soft, keywords, have_hard, have_soft, gaps)\n",
    "\n",
    "skills_out = gen_skills(CTX)\n",
    "cover_out  = gen_cover(CTX)\n",
    "emails_out = gen_emails(CTX)\n",
    "ats_out    = gen_ats(CTX)\n",
    "\n",
    "# 7) show and save\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "(OUT_DIR / f\"{ts}_skills_keywords.md\").write_text(skills_out, encoding=\"utf-8\")\n",
    "(OUT_DIR / f\"{ts}_cover_letter.md\").write_text(cover_out, encoding=\"utf-8\")\n",
    "(OUT_DIR / f\"{ts}_emails.md\").write_text(emails_out, encoding=\"utf-8\")\n",
    "(OUT_DIR / f\"{ts}_ats_summary.md\").write_text(ats_out, encoding=\"utf-8\")\n",
    "\n",
    "print(\"=== SKILLS & KEYWORDS ===\\n\", skills_out, \"\\n\")\n",
    "print(\"=== COVER LETTER ===\\n\", cover_out, \"\\n\")\n",
    "print(\"=== EMAILS ===\\n\", emails_out, \"\\n\")\n",
    "print(\"=== ATS SUMMARY ===\\n\", ats_out, \"\\n\")\n",
    "print(\"Saved files to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48a1d7-993c-47a4-8aa5-75ca285f27c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
